"""
Exploit Classifier - Machine learning-based exploit detection

This module implements ML models for classifying transactions as benign or malicious
based on extracted features. It uses ensemble methods (Random Forest, Gradient Boosting)
to achieve high accuracy and low false positive rates.

Key Features:
- Random Forest and Gradient Boosting classifiers
- Feature importance analysis
- Real-time prediction with confidence scores
- Model training from historical exploit data
- Cross-validation and performance metrics
- Threshold tuning for false positive control
- Online learning capabilities
- Explainable predictions

Author: SpoonOS Skills
Category: Web3 Data Intelligence - Security Analysis
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import pickle
import os

import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.preprocessing import StandardScaler

from payload_analyzer import PayloadFeatures


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ThreatLevel(Enum):
    """Threat level classification"""
    BENIGN = "benign"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class ClassificationResult:
    """Result of exploit classification"""
    is_malicious: bool
    confidence: float  # 0.0 to 1.0
    threat_level: ThreatLevel
    probability_malicious: float
    probability_benign: float
    feature_importances: Dict[str, float]
    decision_factors: List[str]  # Top factors influencing decision
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'is_malicious': self.is_malicious,
            'confidence': self.confidence,
            'threat_level': self.threat_level.value,
            'probability_malicious': self.probability_malicious,
            'probability_benign': self.probability_benign,
            'feature_importances': self.feature_importances,
            'decision_factors': self.decision_factors
        }


@dataclass
class ModelMetrics:
    """Model performance metrics"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    roc_auc: float
    false_positive_rate: float
    false_negative_rate: float
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'accuracy': self.accuracy,
            'precision': self.precision,
            'recall': self.recall,
            'f1_score': self.f1_score,
            'roc_auc': self.roc_auc,
            'false_positive_rate': self.false_positive_rate,
            'false_negative_rate': self.false_negative_rate
        }


class ExploitClassifier:
    """
    ML-based exploit classifier using ensemble methods
    
    This class trains and applies machine learning models to classify
    transactions as benign or malicious based on extracted features.
    Supports multiple ensemble methods and provides explainable predictions.
    """
    
    # Feature names (22 dimensions)
    FEATURE_NAMES = [
        'function_selector_entropy',
        'has_unknown_function',
        'has_zero_address',
        'has_max_uint',
        'has_small_values',
        'has_large_values',
        'parameter_count',
        'parameter_complexity',
        'log_value_wei',
        'is_zero_value',
        'gas_price_gwei',
        'gas_price_percentile',
        'gas_price_anomaly',
        'estimated_call_depth',
        'has_delegatecall_pattern',
        'has_external_call_pattern',
        'has_state_change_pattern',
        'matches_reentrancy_pattern',
        'matches_flash_loan_pattern',
        'matches_price_manipulation_pattern',
        'sender_is_new',
        'unusual_timing'
    ]
    
    def __init__(
        self,
        model_type: str = 'random_forest',
        threshold: float = 0.7,
        n_estimators: int = 100,
        random_state: int = 42
    ):
        """
        Initialize exploit classifier
        
        Args:
            model_type: 'random_forest' or 'gradient_boosting'
            threshold: Classification threshold (0.0 to 1.0)
            n_estimators: Number of trees in ensemble
            random_state: Random seed for reproducibility
        """
        self.model_type = model_type
        self.threshold = threshold
        self.n_estimators = n_estimators
        self.random_state = random_state
        
        # Initialize model
        if model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=random_state,
                n_jobs=-1  # Use all CPU cores
            )
        elif model_type == 'gradient_boosting':
            self.model = GradientBoostingClassifier(
                n_estimators=n_estimators,
                max_depth=5,
                learning_rate=0.1,
                random_state=random_state
            )
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # Feature scaler for normalization
        self.scaler = StandardScaler()
        
        # Model state
        self.is_trained = False
        self.feature_importances_ = None
        
        logger.info(f"ExploitClassifier initialized with {model_type} ({n_estimators} estimators)")
    
    def train(
        self,
        X: np.ndarray,
        y: np.ndarray,
        validation_split: float = 0.2
    ) -> ModelMetrics:
        """
        Train classifier on labeled data
        
        Args:
            X: Feature matrix (n_samples, 22)
            y: Labels (0=benign, 1=malicious)
            validation_split: Fraction of data for validation
        
        Returns:
            ModelMetrics with performance results
        """
        logger.info(f"Training {self.model_type} classifier on {len(X)} samples")
        
        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X, y,
            test_size=validation_split,
            random_state=self.random_state,
            stratify=y
        )
        
        # Normalize features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        # Train model
        self.model.fit(X_train_scaled, y_train)
        self.is_trained = True
        
        # Calculate feature importances
        if hasattr(self.model, 'feature_importances_'):
            self.feature_importances_ = self.model.feature_importances_
        
        # Evaluate on validation set
        y_pred = self.model.predict(X_val_scaled)
        y_pred_proba = self.model.predict_proba(X_val_scaled)[:, 1]
        
        # Calculate metrics
        metrics = self._calculate_metrics(y_val, y_pred, y_pred_proba)
        
        logger.info(f"âœ… Training complete - Accuracy: {metrics.accuracy:.3f}, F1: {metrics.f1_score:.3f}")
        
        return metrics
    
    def predict(self, features: PayloadFeatures) -> ClassificationResult:
        """
        Predict if transaction is malicious
        
        Args:
            features: PayloadFeatures extracted from transaction
        
        Returns:
            ClassificationResult with prediction and confidence
        """
        if not self.is_trained:
            raise RuntimeError("Model not trained. Call train() first.")
        
        # Convert features to numpy array
        X = np.array(features.to_feature_vector()).reshape(1, -1)
        
        # Normalize
        X_scaled = self.scaler.transform(X)
        
        # Predict probabilities
        proba = self.model.predict_proba(X_scaled)[0]
        prob_benign = proba[0]
        prob_malicious = proba[1]
        
        # Classify based on threshold
        is_malicious = prob_malicious >= self.threshold
        
        # Determine threat level
        threat_level = self._determine_threat_level(prob_malicious)
        
        # Calculate confidence (distance from decision boundary)
        confidence = abs(prob_malicious - self.threshold) / (1.0 - self.threshold)
        confidence = min(1.0, max(0.0, confidence))
        
        # Get feature importances for this prediction
        feature_importances = self._get_feature_importances()
        
        # Identify decision factors (top contributing features)
        decision_factors = self._get_decision_factors(features, feature_importances)
        
        result = ClassificationResult(
            is_malicious=is_malicious,
            confidence=confidence,
            threat_level=threat_level,
            probability_malicious=prob_malicious,
            probability_benign=prob_benign,
            feature_importances=feature_importances,
            decision_factors=decision_factors
        )
        
        return result
    
    def predict_batch(self, features_list: List[PayloadFeatures]) -> List[ClassificationResult]:
        """Predict on batch of transactions"""
        return [self.predict(features) for features in features_list]
    
    def _calculate_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_pred_proba: np.ndarray
    ) -> ModelMetrics:
        """Calculate model performance metrics"""
        # Basic metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        # Confusion matrix
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        # False positive/negative rates
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0
        
        return ModelMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1,
            roc_auc=roc_auc,
            false_positive_rate=fpr,
            false_negative_rate=fnr
        )
    
    def _determine_threat_level(self, probability: float) -> ThreatLevel:
        """Determine threat level from malicious probability"""
        if probability < 0.3:
            return ThreatLevel.BENIGN
        elif probability < 0.5:
            return ThreatLevel.LOW
        elif probability < 0.7:
            return ThreatLevel.MEDIUM
        elif probability < 0.9:
            return ThreatLevel.HIGH
        else:
            return ThreatLevel.CRITICAL
    
    def _get_feature_importances(self) -> Dict[str, float]:
        """Get feature importance scores"""
        if self.feature_importances_ is None:
            return {}
        
        importances = {}
        for i, name in enumerate(self.FEATURE_NAMES):
            if i < len(self.feature_importances_):
                importances[name] = float(self.feature_importances_[i])
        
        return importances
    
    def _get_decision_factors(
        self,
        features: PayloadFeatures,
        importances: Dict[str, float],
        top_k: int = 5
    ) -> List[str]:
        """Identify top factors influencing decision"""
        # Get feature vector
        feature_vec = features.to_feature_vector()
        
        # Calculate contribution (importance * feature_value)
        contributions = []
        for i, (name, importance) in enumerate(importances.items()):
            if i < len(feature_vec):
                contribution = importance * abs(feature_vec[i])
                contributions.append((name, contribution))
        
        # Sort by contribution
        contributions.sort(key=lambda x: x[1], reverse=True)
        
        # Return top K factor names
        return [name for name, _ in contributions[:top_k]]
    
    def save_model(self, filepath: str):
        """Save trained model to file"""
        if not self.is_trained:
            raise RuntimeError("Cannot save untrained model")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_importances': self.feature_importances_,
            'threshold': self.threshold,
            'model_type': self.model_type
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """Load trained model from file"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_importances_ = model_data['feature_importances']
        self.threshold = model_data['threshold']
        self.model_type = model_data['model_type']
        self.is_trained = True
        
        logger.info(f"Model loaded from {filepath}")
    
    def tune_threshold(
        self,
        X_val: np.ndarray,
        y_val: np.ndarray,
        target_fpr: float = 0.01
    ) -> float:
        """
        Tune classification threshold to achieve target false positive rate
        
        Args:
            X_val: Validation features
            y_val: Validation labels
            target_fpr: Target false positive rate (e.g., 0.01 = 1%)
        
        Returns:
            Optimal threshold
        """
        X_val_scaled = self.scaler.transform(X_val)
        y_pred_proba = self.model.predict_proba(X_val_scaled)[:, 1]
        
        # Try different thresholds
        best_threshold = self.threshold
        best_diff = float('inf')
        
        for threshold in np.arange(0.5, 1.0, 0.01):
            y_pred = (y_pred_proba >= threshold).astype(int)
            tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
            
            diff = abs(fpr - target_fpr)
            if diff < best_diff:
                best_diff = diff
                best_threshold = threshold
        
        self.threshold = best_threshold
        logger.info(f"Threshold tuned to {best_threshold:.3f} (target FPR: {target_fpr:.3f})")
        
        return best_threshold
    
    def generate_synthetic_training_data(
        self,
        n_benign: int = 1000,
        n_malicious: int = 200
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generate synthetic training data for bootstrapping
        
        Args:
            n_benign: Number of benign samples
            n_malicious: Number of malicious samples
        
        Returns:
            Tuple of (X, y) - features and labels
        """
        logger.info(f"Generating synthetic data: {n_benign} benign, {n_malicious} malicious")
        
        # Benign transaction features (typical values)
        X_benign = np.random.randn(n_benign, 22)
        X_benign[:, 0] = np.random.uniform(2.0, 4.0, n_benign)  # Normal entropy
        X_benign[:, 1] = np.random.binomial(1, 0.1, n_benign)  # 10% unknown functions
        X_benign[:, 2:6] = np.random.binomial(1, 0.05, (n_benign, 4))  # Rare suspicious params
        X_benign[:, 7] = np.random.uniform(0.1, 0.5, n_benign)  # Low-medium complexity
        X_benign[:, 10] = np.random.uniform(10, 50, n_benign)  # Normal gas price
        X_benign[:, 11] = np.random.uniform(20, 80, n_benign)  # Normal percentile
        X_benign[:, 12] = np.random.binomial(1, 0.05, n_benign)  # 5% gas anomalies
        X_benign[:, 17:20] = 0  # No exploit patterns
        
        # Malicious transaction features (anomalous values)
        X_malicious = np.random.randn(n_malicious, 22)
        X_malicious[:, 0] = np.random.uniform(1.0, 2.5, n_malicious)  # Low entropy
        X_malicious[:, 1] = np.random.binomial(1, 0.7, n_malicious)  # 70% unknown functions
        X_malicious[:, 2:6] = np.random.binomial(1, 0.5, (n_malicious, 4))  # Many suspicious params
        X_malicious[:, 7] = np.random.uniform(0.6, 1.0, n_malicious)  # High complexity
        X_malicious[:, 10] = np.random.uniform(50, 500, n_malicious)  # High gas price
        X_malicious[:, 11] = np.random.uniform(85, 100, n_malicious)  # High percentile
        X_malicious[:, 12] = np.random.binomial(1, 0.8, n_malicious)  # 80% gas anomalies
        X_malicious[:, 17:20] = np.random.binomial(1, 0.6, (n_malicious, 3))  # Exploit patterns
        
        # Combine and create labels
        X = np.vstack([X_benign, X_malicious])
        y = np.hstack([np.zeros(n_benign), np.ones(n_malicious)])
        
        # Shuffle
        indices = np.random.permutation(len(X))
        X = X[indices]
        y = y[indices]
        
        return X, y



